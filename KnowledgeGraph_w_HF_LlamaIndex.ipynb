{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyOjjowmJ5vBP2Otvv4As5Bp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antaripg/knowledgeGraphWLlamaIndex/blob/main/KnowledgeGraph_w_HF_LlamaIndex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oN_20u9Fwwuk",
        "outputId": "8fc71d32-ba98-4f0d-f8b2-7078efec37de"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.5/15.5 MB\u001b[0m \u001b[31m85.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.3/990.3 kB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.3/377.3 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.0/337.0 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.1/315.1 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q llama_index pyvis Ipython langchain pypdf\n",
        "!pip install -q llama-index-llms-huggingface\n",
        "!pip install -q llama-index-llms-huggingface-api\n",
        "!pip install -q llama-index-embeddings-langchain\n",
        "!pip install -q llama-index\n",
        "!pip install -q --upgrade llama-index-readers-wikipedia\n",
        "!pip install -q --upgrade sentence-transformers\n",
        "!pip install -q --upgrade accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade --quiet langchain langchain-community langchain-experimental wikipedia\n",
        "!pip install -q --upgrade llama-index-llms-langchain\n",
        "!pip install -q --upgrade --quiet neo4j wikipedia tiktoken\n",
        "!pip install -q --upgrade --quiet transformers\n",
        "!pip install -q --upgrade --quiet bitsandbytes==0.41.3 datasets==2.12.0 #transformers==4.37.2\n",
        "!pip install -q --upgrade --quiet json-repair\n",
        "!pip install -q --upgrade llama-index-embeddings-huggingface\n",
        "# !pip install -q --upgrade llama-index-embeddings-instructor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JK7UTGlOxyKG",
        "outputId": "995c694e-fb4f-4f52-bbf0-2832f29d5afd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.2/203.2 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.7/293.7 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m100.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m853.2/853.2 kB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "5FfRTk02x_yh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# System and logging libraries\n",
        "import logging\n",
        "import sys\n",
        "import os\n",
        "from google.colab import userdata\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Logging Configuration\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
        "\n",
        "import accelerate\n",
        "import transformers\n",
        "import torch\n",
        "\n",
        "# LlamaIndex Libraries\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "from llama_index.core import KnowledgeGraphIndex\n",
        "from llama_index.core import Settings\n",
        "from llama_index.core.graph_stores import SimpleGraphStore\n",
        "from llama_index.core import StorageContext\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.readers.wikipedia import WikipediaReader\n",
        "from llama_index.embeddings.langchain import LangchainEmbedding\n",
        "from llama_index.llms.langchain import LangChainLLM\n",
        "from llama_index.core.prompts import PromptTemplate\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "\n",
        "# Langchain embedding libraries\n",
        "from langchain.embeddings import HuggingFaceEmbeddings, HuggingFaceInferenceAPIEmbeddings\n",
        "\n",
        "# Visualization Libraries\n",
        "from pyvis.network import Network\n",
        "from IPython.display import display, HTML\n",
        "from pyvis.network import Network\n",
        "\n",
        "# Direct Model Loading form HuggingFace\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline, logging\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "# from langchain.embeddings.huggingface import HuggingFaceEmbeddings"
      ],
      "metadata": {
        "id": "xnodJ8NmyZFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the API KEYS\n",
        "# os.environ['HUGGINGFACEHUB_API_TOKEN'] = userdata.get('HUGGINGFACE_API_KEY')\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = userdata.get('HUGGINGFACE_API_KEY_2')"
      ],
      "metadata": {
        "id": "VpoTJ4p2ya3z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "outputId": "5d7f8a1f-97ed-4882-e9ea-0abf3de98046"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TimeoutException",
          "evalue": "Requesting secret HUGGINGFACE_API_KEY_2 timed out. Secrets can only be fetched when running from the Colab UI.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTimeoutException\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-4800db655af4>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Get the API KEYS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# os.environ['HUGGINGFACEHUB_API_TOKEN'] = userdata.get('HUGGINGFACE_API_KEY')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'HUGGINGFACEHUB_API_TOKEN'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'HUGGINGFACE_API_KEY_2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/userdata.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m     64\u001b[0m     )\n\u001b[1;32m     65\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exists'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mSecretNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTimeoutException\u001b[0m: Requesting secret HUGGINGFACE_API_KEY_2 timed out. Secrets can only be fetched when running from the Colab UI."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up the LLM and Embedding Model"
      ],
      "metadata": {
        "id": "FFF2mJ0Ny_rS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "\n",
        "online_llm = HuggingFaceInferenceAPI(model_name=MODEL_NAME,\n",
        "                              token=os.environ['HUGGINGFACEHUB_API_TOKEN'])\n",
        "\n",
        "\n",
        "# KG_MODEL_NAME = \"SciPhi/Triplex\"\n",
        "# kg_llm = HuggingFaceInferenceAPI(model_name=KG_MODEL_NAME,\n",
        "#                              token=os.environ['HUGGINGFACE_API_TOKEN'])"
      ],
      "metadata": {
        "id": "VLMKxQ9ay_AM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBED_MODEL = \"thenlper/gte-large\"\n",
        "online_embed_model = LangchainEmbedding(\n",
        "  HuggingFaceInferenceAPIEmbeddings(api_key=os.environ['HUGGINGFACEHUB_API_TOKEN'],\n",
        "                                    model_name=EMBED_MODEL)\n",
        ")"
      ],
      "metadata": {
        "id": "IFVizl0Ry9Na"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download LLM Model"
      ],
      "metadata": {
        "id": "Ac6qT7lvQ_Gz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def messages_to_prompt(messages):\n",
        "  prompt = \"\"\n",
        "  for message in messages:\n",
        "    if message.role == 'system':\n",
        "      prompt += f\"<|system|>\\n{message.content}</s>\\n\"\n",
        "    elif message.role == 'user':\n",
        "      prompt += f\"<|user|>\\n{message.content}</s>\\n\"\n",
        "    elif message.role == 'assistant':\n",
        "      prompt += f\"<|assistant|>\\n{message.content}</s>\\n\"\n",
        "\n",
        "  # ensure we start with a system prompt, insert blank if needed\n",
        "  if not prompt.startswith(\"<|system|>\\n\"):\n",
        "    prompt = \"<|system|>\\n</s>\\n\" + prompt\n",
        "\n",
        "  # add final assistant prompt\n",
        "  prompt = prompt + \"<|assistant|>\\n\"\n",
        "\n",
        "  return prompt\n",
        "\n",
        "def completion_to_prompt(completion):\n",
        "    return f\"<|system|>\\n</s>\\n<|user|>\\n{completion}</s>\\n<|assistant|>\\n\"\n",
        "\n",
        "\n",
        "# Model Name\n",
        "model_name = \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "\n",
        "# Quantization Parameters\n",
        "\n",
        "use_4bit = True # Activate 4-bit precision base model loading\n",
        "bnb_4bit_compute_dtype = \"float16\" # Compute dtype for 4-bit base models\n",
        "bnb_4bit_quant_type = \"nf4\" # Quantization type (fp4 or nf4)\n",
        "use_nested_quant = False # Activate nested quantization for 4-bit base models (double quantization)\n",
        "use_optimizer = False # Activate optimizer for 4-bit base models (double quantization)\n",
        "\n",
        "# Quantization config\n",
        "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
        "# Load the entire model on the GPU 0\n",
        "device_map = {\"\": 0}\n",
        "\n",
        "# Check GPU compatibility with bfloat16\n",
        "if compute_dtype == torch.float16 and use_4bit:\n",
        "    major, _ = torch.cuda.get_device_capability()\n",
        "    if major >= 8:\n",
        "        print(\"=\" * 80)\n",
        "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,\n",
        ")\n",
        "\n",
        "# Create the LLM\n",
        "offline_llm = HuggingFaceLLM(\n",
        "    model_name=\"HuggingFaceH4/zephyr-7b-beta\",\n",
        "    tokenizer_name=\"HuggingFaceH4/zephyr-7b-beta\",\n",
        "    context_window=3900,\n",
        "    max_new_tokens=256,\n",
        "    model_kwargs={\"quantization_config\": bnb_config},\n",
        "    generate_kwargs={\"temperature\": 0.3, \"top_k\": 50, \"top_p\": 0.95, \"pad_token_id\": 1},\n",
        "    messages_to_prompt=messages_to_prompt,\n",
        "    completion_to_prompt=completion_to_prompt,\n",
        "    tokenizer_kwargs={\"eos_token_id\": 1, \"padding_side\": \"left\"},\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "# llm.tokenizer_kwargs[\"padding_side\"] = \"left\"\n",
        "# llm.tokenizer_kwargs[\"\"]\n",
        "# response = llm.complete(\"What is the meaning of life?\")\n",
        "# print(str(response))"
      ],
      "metadata": {
        "id": "iOHGpuMN6L3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Load Embedding Model Locally\n",
        "# loads BAAI/bge-small-en-v1.5\n",
        "embed_model_name = \"thenlper/gte-large\"\n",
        "offline_embed_model = HuggingFaceEmbedding(model_name=embed_model_name)"
      ],
      "metadata": {
        "id": "6V-NvFWp9OmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reading Documents"
      ],
      "metadata": {
        "id": "DRfWsOUl5quU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GET documents from wikipedia\n",
        "def get_documents_from_wikipedia(a_document_seach_string):\n",
        "  # Reader\n",
        "  reader = WikipediaReader()\n",
        "  # Load Data From Wikipedia\n",
        "  r_documents = reader.load_data(pages=[a_document_seach_string])\n",
        "  return r_documents\n",
        "\n",
        "# SET GRAPH STORE and STORAGE CONTEXT\n",
        "def set_graph_store_and_storage_context():\n",
        "  # Set Up the storage context\n",
        "  graph_store = SimpleGraphStore()\n",
        "  r_storage_context = StorageContext.from_defaults(graph_store=graph_store)\n",
        "  return r_storage_context\n",
        "\n",
        "# CREATE the knowledge index\n",
        "def create_kg_index(a_documents, a_storage_context, a_kg_llm, a_embed_model):\n",
        "  # Define and construct knowledge graph index\n",
        "  r_index = KnowledgeGraphIndex.from_documents(\n",
        "      a_documents,\n",
        "      storage_context=a_storage_context,\n",
        "      max_triplets_per_chunk=3,\n",
        "      embed_model=a_embed_model,\n",
        "      llm=a_kg_llm,\n",
        "      include_embeddings=True\n",
        "  )\n",
        "  return r_index\n",
        "\n",
        "# CREATE the query engine\n",
        "def create_query_engine(a_index, a_llm):\n",
        "  # Query Engine\n",
        "  r_query_engine = index.as_query_engine(include_text=True,\n",
        "                                     response_mode=\"tree_summarize\",\n",
        "                                     embedding_mode=\"hybrid\",\n",
        "                                     similarity_top_k=5, llm=a_llm)\n",
        "  return r_query_engine\n",
        "\n",
        "# VISUALIZE the knowledge graph\n",
        "def visualize_knowledge_graph(a_index, a_document_seach_string, a_show=True):\n",
        "  g = a_index.get_networkx_graph()\n",
        "  net = Network(notebook=True, cdn_resources='in_line', directed=True)\n",
        "  net.from_nx(g)\n",
        "  # net.show(\"graph.html\")\n",
        "  net.save_graph(f\"{a_document_seach_string}_knowledge_graph.html\")\n",
        "  if a_show:\n",
        "    display(HTML(filename=f'/content/{a_document_seach_string}_knowledge_graph.html'))\n",
        "  return 0\n",
        "\n",
        "# CREATE query prompt\n",
        "def create_query_prompt(a_query: str):\n",
        "  query = a_query\n",
        "  # message template\n",
        "  r_message_template =f\"\"\"<|system|>Please check if the following pieces of context has any mention of the keywords provided in the Question. If not then don't know the answer, just say that you don't know. Stop there. Please donot try to make up an answer.</s>\n",
        "  <|user|>\n",
        "  Question: {query}\n",
        "  Helpful Answer:\n",
        "  </s>\"\"\"\n",
        "  return r_message_template\n",
        "\n",
        "# GET Query prompt\n",
        "def get_response(a_query: str, a_query_engine):\n",
        "  a_query_prompt = create_query_prompt(a_query)\n",
        "  r_response = a_query_engine.query(a_query_prompt)\n",
        "  return r_response"
      ],
      "metadata": {
        "id": "5SPBHO4qJRSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up the service Context (global setting of LLM)\n",
        "Settings.llm = online_llm\n",
        "Settings.chunk_size = 512\n",
        "# Settings.embed_model = embed_model"
      ],
      "metadata": {
        "id": "ErW_zwRzG3gK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading from Wikipedia\n",
        "document_search_string = 'Charles, Prince of Wales'\n",
        "documents = get_documents_from_wikipedia(document_search_string)\n",
        "\n",
        "# Set up the storage context\n",
        "storage_context = set_graph_store_and_storage_context()\n",
        "\n",
        "# Construct the Knowledge Graph Index\n",
        "online_index = create_kg_index(documents, storage_context, online_llm, online_embed_model)\n",
        "\n",
        "# Query Engine\n",
        "online_query_engine = create_query_engine(online_index, online_llm)\n",
        "\n",
        "# Visualization\n",
        "_ = visualize_knowledge_graph(online_index, document_search_string)"
      ],
      "metadata": {
        "id": "2Bj9qB2VJpoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading from Wikipedia\n",
        "document_search_string = 'Charles, Prince of Wales'\n",
        "documents = get_documents_from_wikipedia(document_search_string)\n",
        "\n",
        "# Set up the storage context\n",
        "storage_context = set_graph_store_and_storage_context()\n",
        "\n",
        "# Construct the Knowledge Graph Index\n",
        "offline_index = create_kg_index(documents, storage_context, offline_llm, offline_embed_model)\n",
        "\n",
        "# Offline Query Engine\n",
        "offline_query_engine = create_query_engine(offline_index, offline_llm)\n",
        "\n",
        "# Hybrid Query Engine\n",
        "hybrid_query_engine = create_query_engine(offline_index, online_llm)\n",
        "\n",
        "# Visualization\n",
        "_ = visualize_knowledge_graph(offline_index, document_search_string)"
      ],
      "metadata": {
        "id": "1C9OmWGvJNJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# query_list = [\"How is Elizabeth I and Mary related?\",\n",
        "#               \"When was Elizabeth I and Mary born?\",\n",
        "#               \"What languages did she speak?\",\n",
        "#               \"What languages did Elizabeth I speak?\",\n",
        "#               \"What was Elizabeth I's occupation?\",\n",
        "#               \"When was she baptized?\",\n",
        "#               \"How was the relationship between Elizabeth I and Mary?\"]\n"
      ],
      "metadata": {
        "id": "k54HlTkNKHId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Response\n",
        "query_list = [\"Who married Camilla Parker Bowles?\",\n",
        "              \"Who is married to Camilla Parker Bowles?\",\n",
        "              \"Where did the Camilla and her husband move to?\"]\n",
        "for query in query_list:\n",
        "\n",
        "  print(f\"Question: {query}\")\n",
        "\n",
        "  # Complete Online Answer\n",
        "  response = get_response(a_query=query, a_query_engine=online_query_engine)\n",
        "  print(f\"Online Answer: {response}\")\n",
        "  print(\"\\n\\n\")\n",
        "  try: # Complete offline Answer\n",
        "    response = get_response(a_query=query, a_query_engine=offline_query_engine)\n",
        "    print(f\"Offline Answer: {response}\")\n",
        "  except Exception as e: # Using a Hybrid Approach if OutOfMemory Error\n",
        "    response = get_response(a_query=query, a_query_engine=hyrbid_query_engine)\n",
        "    print(f\"Hybrid Answer: {response}\")\n",
        "\n",
        "  print(\"------------------------------------------------------------\")\n",
        "  print(\"\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsiFSNiWcsAE",
        "outputId": "d3979009-cded-4fb8-a58a-c9c1a06feac3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Who married Camilla Parker Bowles?\n",
            "Online Answer: \n",
            "<|assistant|>\n",
            "Charles III, the current King of the United Kingdom and the 14 other Commonwealth realms, married Camilla Parker Bowles.\n",
            "\n",
            "Context information from multiple sources:\n",
            "\n",
            "- \"Prince Charles and Camilla Parker Bowles were married in a civil ceremony at Windsor Guildhall followed by a Church of England service at St. George's Chapel, Windsor Castle, on 9 April 2005.\" (Britannica.com, 2021)\n",
            "- \"Prince Charles and Camilla Parker Bowles were married in a civil ceremony at Windsor Guildhall followed by a Church of England service at St. George's Chapel, Windsor Castle, on 9 April 2005.\" (Royal.uk, 2021)\n",
            "- \"Prince Charles and Camilla Parker Bowles were married in a civil ceremony at Windsor Guildhall followed by a Church of England service at St. George's Chapel, Windsor Castle, on 9 April 2005.\" (Bbc.com, 2021)\n",
            "- \"Prince Charles and Camilla Parker Bow\n",
            "Offline Answer: \n",
            "<|assistant|>\n",
            "Charles III, the current King of the United Kingdom and the 14 other Commonwealth realms, married Camilla Parker Bowles.\n",
            "\n",
            "Context information from multiple sources:\n",
            "\n",
            "- \"Prince Charles and Camilla Parker Bowles were married in a civil ceremony at Windsor Guildhall followed by a Church of England service at St. George's Chapel, Windsor Castle, on 9 April 2005.\" (Britannica.com, 2021)\n",
            "- \"Prince Charles and Camilla Parker Bowles were married in a civil ceremony at Windsor Guildhall followed by a Church of England service at St. George's Chapel, Windsor Castle, on 9 April 2005.\" (Royal.uk, 2021)\n",
            "- \"Prince Charles and Camilla Parker Bowles were married in a civil ceremony at Windsor Guildhall followed by a Church of England service at St. George's Chapel, Windsor Castle, on 9 April 2005.\" (Bbc.com, 2021)\n",
            "- \"Prince Charles and Camilla Parker Bow\n",
            "------------------------------------------------------------\n",
            "\n",
            "\n",
            "\n",
            "Question: Who is married to Camilla Parker Bowles?\n",
            "Online Answer: \n",
            "<|assistant|>\n",
            "Charles III, the current King of the United Kingdom and other Commonwealth realms, is married to Camilla Parker Bowles. (Source: Provided context)\n",
            "\n",
            "If the question had asked about someone else, and their relationship status with Camilla Parker Bowles was not mentioned in the context, then the answer would be \"I don't know.\" (Source: Instruction provided)\n",
            "\n",
            "It's important not to make up an answer, as that could lead to incorrect information being spread. (Source: Instruction provided)\n",
            "\n",
            "Stop there. (Source: Instruction provided)\n",
            "Offline Answer: \n",
            "<|assistant|>\n",
            "Charles III, the current King of the United Kingdom and other Commonwealth realms, is married to Camilla Parker Bowles. (Source: Provided context)\n",
            "\n",
            "If the question had asked about someone else, and their relationship status with Camilla Parker Bowles was not mentioned in the context, then the answer would be \"I don't know.\" (Source: Instruction provided)\n",
            "\n",
            "It's important not to make up an answer, as that could lead to incorrect information being spread. (Source: Instruction provided)\n",
            "\n",
            "Stop there. (Source: Instruction provided)\n",
            "------------------------------------------------------------\n",
            "\n",
            "\n",
            "\n",
            "Question: Where did the Camilla and her husband move to?\n",
            "Online Answer: \n",
            "<|assistant|>\n",
            "Based on the provided context, there is no mention of Camilla having a husband. In fact, the context only refers to Camilla Parker Bowles and her relationship with Prince Charles. The context does not provide information about any marriage or husband for Camilla. Therefore, I do not know the answer to your question.\n",
            "Offline Answer: \n",
            "<|assistant|>\n",
            "Based on the provided context, there is no mention of Camilla having a husband. In fact, the context only refers to Camilla Parker Bowles and her relationship with Prince Charles. The context does not provide information about any marriage or husband for Camilla. Therefore, I do not know the answer to your question.\n",
            "------------------------------------------------------------\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating Another Knowledge Graph"
      ],
      "metadata": {
        "id": "2Fh77eSTECzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "del index\n",
        "del storage_context\n",
        "del query_engine\n",
        "del documents\n",
        "del document_search_string\n",
        "del query_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "UbYZwsvhWO4r",
        "outputId": "92140715-de2c-452d-bf5e-65a449c28a72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'query_list' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-013055f5e52a>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mstorage_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mquery_engine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mquery_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mdocument_search_string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'query_list' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading from Wikipedia\n",
        "document_search_string = 'Elizabeth II'\n",
        "\n",
        "\n",
        "# Load Data From Wikipedia\n",
        "documents = get_documents_from_wikipedia(a_document_seach_string=[document_search_string])\n",
        "\n",
        "# Set up the storage context\n",
        "storage_context = set_graph_store_and_storage_context()\n",
        "\n",
        "# Construct the Knowledge Graph Index\n",
        "index = create_kg_index(documents, storage_context, llm)\n",
        "\n",
        "# Query Engine\n",
        "query_engine = create_query_engine(index)\n",
        "\n",
        "# Visualization\n",
        "_ = visualize_knowledge_graph(index, document_search_string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "S8XbmipeAhh5",
        "outputId": "41627168-01a3-427d-81fe-99a4715c97e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValidationError",
          "evalue": "1 validation error for EmbeddingEndEvent\nembeddings\n  value is not a valid list (type=type_error.list)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-7f90f3610cb7>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Construct the Knowledge Graph Index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_kg_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Query Engine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-809e8b876f14>\u001b[0m in \u001b[0;36mcreate_kg_index\u001b[0;34m(a_documents, a_storage_context, a_kg_llm)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_kg_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_storage_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_kg_llm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0;31m# Define and construct knowledge graph index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m   r_index = KnowledgeGraphIndex.from_documents(\n\u001b[0m\u001b[1;32m     20\u001b[0m       \u001b[0ma_documents\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m       \u001b[0mstorage_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma_storage_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/core/indices/base.py\u001b[0m in \u001b[0;36mfrom_documents\u001b[0;34m(cls, documents, storage_context, show_progress, callback_manager, transformations, service_context, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m             )\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             return cls(\n\u001b[0m\u001b[1;32m    146\u001b[0m                 \u001b[0mnodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0mstorage_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/core/indices/knowledge_graph/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, nodes, objects, index_struct, llm, embed_model, storage_context, kg_triplet_extract_template, max_triplets_per_chunk, include_embeddings, show_progress, max_object_length, kg_triplet_extract_fn, service_context, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m         )\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    110\u001b[0m             \u001b[0mnodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mindex_struct\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex_struct\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/core/indices/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, nodes, objects, index_struct, storage_context, callback_manager, transformations, show_progress, service_context, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mindex_struct\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0mnodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnodes\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 index_struct = self.build_index_from_nodes(\n\u001b[0m\u001b[1;32m     95\u001b[0m                     \u001b[0mnodes\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mobjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/core/indices/base.py\u001b[0m in \u001b[0;36mbuild_index_from_nodes\u001b[0;34m(self, nodes, **build_kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;34m\"\"\"Build the index from nodes.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_docstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_update\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_index_from_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mbuild_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/core/indices/knowledge_graph/base.py\u001b[0m in \u001b[0;36m_build_index_from_nodes\u001b[0;34m(self, nodes)\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0mtriplet_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtriplets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m                 embed_outputs = self._embed_model.get_text_embedding_batch(\n\u001b[0m\u001b[1;32m    228\u001b[0m                     \u001b[0mtriplet_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_progress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    228\u001b[0m             )\n\u001b[1;32m    229\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSpanDropEvent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspan_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mid_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/core/base/embeddings/base.py\u001b[0m in \u001b[0;36mget_text_embedding_batch\u001b[0;34m(self, texts, show_progress, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m                     )\n\u001b[1;32m    340\u001b[0m                 dispatcher.event(\n\u001b[0;32m--> 341\u001b[0;31m                     EmbeddingEndEvent(\n\u001b[0m\u001b[1;32m    342\u001b[0m                         \u001b[0mchunks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m                         \u001b[0membeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydantic/v1/main.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__pydantic_self__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidation_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalidation_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mobject_setattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__pydantic_self__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__dict__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValidationError\u001b[0m: 1 validation error for EmbeddingEndEvent\nembeddings\n  value is not a valid list (type=type_error.list)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Query List\n",
        "\n",
        "query_list = []\n",
        "\n",
        "\n",
        "# Get Response\n",
        "for query in query_list:\n",
        "  print(f\"Question: {query}\")\n",
        "  response = get_response(a_query=query, a_query_engine=query_engine)\n",
        "  print(f\"Answer: {response}\")\n",
        "  print(\"------------------------------------------------------------\")\n",
        "  print(\"\\n\\n\")"
      ],
      "metadata": {
        "id": "w5-IIBLiL6mI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}